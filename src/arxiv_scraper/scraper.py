import datetime
import time
from typing import List
from urllib.parse import urljoin

import requests
from bs4 import BeautifulSoup

from .config import settings
from .database import DuckDBManager

# Importa√ß√µes internas
from .models import Article, save_articles_to_csv


def parse_arxiv_article(dt_tag: BeautifulSoup, dd_tag: BeautifulSoup) -> Article:
    """
    Extrai os dados de um par <dt> (ID) e <dd> (Detalhes) da lista do Arxiv
    e retorna um objeto Article validado pelo Pydantic.
    """

    # 1. Extra√ß√£o do ID e Link
    # O ID completo est√° no texto do link 'Abstract'
    arxiv_id_full = dt_tag.find("a", title="Abstract").text.strip()
    arxiv_id = arxiv_id_full.split(":")[-1]  # Ex: "2312.06733"

    print(f"üåê Extraindo detalhes dos artigo: {arxiv_id}")

    # Constr√≥i o link absoluto
    relative_link = dt_tag.find("a", title="Abstract")["href"]
    absolute_link = urljoin("https://arxiv.org/", relative_link)

    # 2. Extra√ß√£o do T√≠tulo
    # O t√≠tulo est√° dentro de uma div com classe 'list-title'
    title_div = dd_tag.find("div", class_="list-title")

    # Remove a descri√ß√£o "Title"
    descriptor = title_div.find("span", class_="descriptor")
    if descriptor:
        descriptor.decompose()

    title = title_div.get_text(" ", strip=True)

    # 3. Extra√ß√£o dos Autores
    # Os autores s√£o links <a> dentro da div 'list-authors'
    author_tags = dd_tag.find("div", class_="list-authors").find_all("a")
    authors = [tag.text.strip() for tag in author_tags]
    authors = ", ".join(authors)

    # 4. Extra√ß√£o de Assuntos
    # O Arxiv agrupa assuntos
    subjects_div = dd_tag.find("div", class_="list-subjects")

    # Remove a descri√ß√£o "Subjects"
    descriptor = subjects_div.find("span", class_="descriptor")
    if descriptor:
        descriptor.decompose()

    subjects = [
        sub.strip() for sub in subjects_div.get_text(" ", strip=True).split(";")
    ]
    subjects_str = ", ".join(subjects)

    # 5. Data de submiss√£o e Resumo
    # Para adquirir essas informa√ß√µes √© necess√°rio fazer um novo request.
    try:
        # 1. Requisi√ß√£o: Timeout definido para boas pr√°ticas
        response = requests.get(absolute_link, timeout=15)
        response.raise_for_status()  # Lan√ßa HTTPError para respostas ruins (4xx, 5xx)
    except requests.exceptions.RequestException as e:
        print(f"‚ùå Erro ao acessar a URL: {e}")
        return []

    # 2. Parsing: Usa o 'html.parser' que √© nativo e r√°pido
    detailed_soup = BeautifulSoup(response.content, "html.parser")

    test_div = (
        detailed_soup.find("div", class_="dateline")
        .get_text(" ", strip=True)
        .replace("Submitted on ", "")
        .strip()
    )

    date_string = test_div[1:-1]
    submission_date = datetime.datetime.strptime(date_string, "%d %b %Y").strftime(
        "%Y-%m-%d"
    )

    abstract_blockquote = detailed_soup.find("blockquote", class_="abstract")

    # Remove a descri√ß√£o "Abstract"
    descriptor = abstract_blockquote.find("span", class_="descriptor")
    if descriptor:
        descriptor.decompose()

    abstract = abstract_blockquote.get_text(" ", strip=True)

    # Retorna o objeto validado (Pydantic garantir√° que os tipos est√£o corretos)
    return Article(
        arxiv_id=arxiv_id,
        title=title,
        authors=authors,
        subjects=subjects_str,
        abstract=abstract,
        link=absolute_link,
        submission_date=submission_date,
    )


def scrape_arxiv(url: str) -> List[Article]:
    """
    Realiza o scraping da p√°gina do Arxiv e retorna
    uma lista de objetos Article validados.
    """
    print(f"üåê Iniciando scraping da URL: {url}")

    try:
        # 1. Requisi√ß√£o: Timeout definido para boas pr√°ticas
        response = requests.get(url, timeout=15)
        response.raise_for_status()  # Lan√ßa HTTPError para respostas ruins (4xx, 5xx)
    except requests.exceptions.RequestException as e:
        print(f"‚ùå Erro ao acessar a URL: {e}")
        return []

    # 2. Parsing: Usa o 'html.parser' que √© nativo e r√°pido
    soup = BeautifulSoup(response.content, "html.parser")

    # O Arxiv usa listas de defini√ß√£o: <dl> para a lista de artigos
    dl_list = soup.find("dl")

    if not dl_list:
        print("Nenhuma lista de artigos (<dl>) encontrada. O scraping falhou.")
        return []

    # Os detalhes de cada artigo est√£o em tags <dt> (ID) e <dd> (Detalhes)
    dt_tags = dl_list.find_all("dt")
    dd_tags = dl_list.find_all("dd")

    if len(dt_tags) != len(dd_tags):
        print(
            "Aviso: N√∫mero de tags <dt> e <dd> n√£o corresponde. "
            "Os dados podem estar incompletos."
        )

    articles: List[Article] = []

    # 3. Extra√ß√£o e Valida√ß√£o em Loop
    for dt, dd in zip(dt_tags, dd_tags, strict=True):  # Corrigido B905
        try:
            # Chama a fun√ß√£o de parsing e valida√ß√£o Pydantic
            article = parse_arxiv_article(dt, dd)
            articles.append(article)
            # break
        except Exception as e:
            # Tratamento de erro espec√≠fico para uma linha,
            # permitindo que o loop continue
            print(f"‚ö†Ô∏è Erro ao parsear um artigo: {e}")
            continue
        time.sleep(settings.REQUEST_DELAY)

    print(f"‚úÖ Scraping conclu√≠do. {len(articles)} artigos extra√≠dos.")
    return articles


def main():
    """Fun√ß√£o principal que orquestra o scraping e a persist√™ncia."""

    # 1. Captura de Dados
    articles = scrape_arxiv(settings.SCRAPE_URL)

    if not articles:
        print("N√£o foi poss√≠vel extrair artigos. Encerrando o processo.")
        return

    # 2. Armazenamento das Informa√ß√µes
    db_manager = DuckDBManager()

    # Design Pattern: Gerenciador de Recurso (DuckDBManager)
    db_manager.insert_articles(articles)
    db_manager.close()

    # Salvar os artigos em um arquivo CSV
    save_articles_to_csv(data=articles)

    print("‚ú® Processo de extra√ß√£o e persist√™ncia conclu√≠do com sucesso.")


if __name__ == "__main__":
    # Ponto de entrada da aplica√ß√£o
    main()
