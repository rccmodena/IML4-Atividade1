# Arquivo: src/arxiv_scraper/scraper.py
import requests
from bs4 import BeautifulSoup
from typing import List
from urllib.parse import urljoin

# Importa√ß√µes internas
from .models import Article
from .config import settings
from .database import DuckDBManager


def parse_arxiv_article(dt_tag: BeautifulSoup, dd_tag: BeautifulSoup) -> Article:
    """
    Extrai os dados de um par <dt> (ID) e <dd> (Detalhes) da lista do Arxiv 
    e retorna um objeto Article validado pelo Pydantic.
    """
    
    # 1. Extra√ß√£o do ID e Link
    # O ID completo est√° no texto do link 'Abstract'
    arxiv_id_full = dt_tag.find('a', title='Abstract').text.strip()
    arxiv_id = arxiv_id_full.split(':')[-1] # Ex: "2312.06733"
    
    # Constr√≥i o link absoluto
    relative_link = dt_tag.find('a', title='Abstract')['href']
    absolute_link = urljoin("https://arxiv.org/", relative_link)

    # 2. Extra√ß√£o do T√≠tulo
    # O t√≠tulo est√° dentro de uma div com classe 'list-title'
    title = dd_tag.find('div', class_='list-title').text.replace('Title: ', '').strip()

    # 3. Extra√ß√£o dos Autores
    # Os autores s√£o links <a> dentro da div 'list-authors'
    author_tags = dd_tag.find('div', class_='list-authors').find_all('a')
    authors = [tag.text.strip() for tag in author_tags]
    
    # 4. Extra√ß√£o de Sujeitos e Data de Submiss√£o
    # O Arxiv agrupa sujeitos e a data de submiss√£o na mesma linha.
    subjects_line = dd_tag.find('div', class_='list-subjects').text.replace('Subjects: ', '').strip()
    
    # A data de submiss√£o est√° no final da linha e √© separada por '[Submitted ...]'
    submission_date = subjects_line.split(' [Submitted ')[-1].replace(']', '')
    subjects = subjects_line.split(' [Submitted ')[0].strip()

    # Observa√ß√£o: O resumo (summary) completo n√£o est√° na p√°gina de lista.
    # Usamos um placeholder para manter o modelo Pydantic completo.
    summary = "Summary not available in list view."

    # Retorna o objeto validado (Pydantic garantir√° que os tipos est√£o corretos)
    return Article(
        arxiv_id=arxiv_id,
        title=title,
        authors=authors,
        subjects=subjects,
        summary=summary,
        link=absolute_link,
        submission_date=submission_date
    )


def scrape_arxiv(url: str) -> List[Article]:
    """Realiza o scraping da p√°gina do Arxiv e retorna uma lista de objetos Article validados."""
    print(f"üåê Iniciando scraping da URL: {url}")
    
    try:
        # 1. Requisi√ß√£o: Timeout definido para boas pr√°ticas
        response = requests.get(url, timeout=15)
        response.raise_for_status() # Lan√ßa HTTPError para respostas ruins (4xx, 5xx)
    except requests.exceptions.RequestException as e:
        print(f"‚ùå Erro ao acessar a URL: {e}")
        return []

    # 2. Parsing: Usa o 'html.parser' que √© nativo e r√°pido
    soup = BeautifulSoup(response.content, 'html.parser')
    
    # O Arxiv usa listas de defini√ß√£o: <dl> para a lista de artigos
    dl_list = soup.find('dl')
    
    if not dl_list:
        print("Nenhuma lista de artigos (<dl>) encontrada. O scraping falhou.")
        return []

    # Os detalhes de cada artigo est√£o em tags <dt> (ID) e <dd> (Detalhes)
    dt_tags = dl_list.find_all('dt')
    dd_tags = dl_list.find_all('dd')
    
    if len(dt_tags) != len(dd_tags):
        print("Aviso: N√∫mero de tags <dt> e <dd> n√£o corresponde. Os dados podem estar incompletos.")

    articles: List[Article] = []
    
    # 3. Extra√ß√£o e Valida√ß√£o em Loop
    for dt, dd in zip(dt_tags, dd_tags):
        try:
            # Chama a fun√ß√£o de parsing e valida√ß√£o Pydantic
            article = parse_arxiv_article(dt, dd)
            articles.append(article)
        except Exception as e:
            # Tratamento de erro espec√≠fico para uma linha, permitindo que o loop continue
            print(f"‚ö†Ô∏è Erro ao parsear um artigo: {e}")
            continue

    print(f"‚úÖ Scraping conclu√≠do. {len(articles)} artigos extra√≠dos.")
    return articles


def main():
    """Fun√ß√£o principal que orquestra o scraping e a persist√™ncia."""
    
    # 1. Captura de Dados
    articles = scrape_arxiv(settings.SCRAPE_URL)

    if not articles:
        print("N√£o foi poss√≠vel extrair artigos. Encerrando o processo.")
        return

    # 2. Armazenamento das Informa√ß√µes
    db_manager = DuckDBManager()
    
    # Design Pattern: Gerenciador de Recurso (DuckDBManager)
    db_manager.insert_articles(articles)
    db_manager.close()
    
    print("‚ú® Processo de extra√ß√£o e persist√™ncia conclu√≠do com sucesso.")


if __name__ == "__main__":
    # Ponto de entrada da aplica√ß√£o
    main()